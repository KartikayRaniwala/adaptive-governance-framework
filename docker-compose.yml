# ============================================================================
# Adaptive Data Governance Framework - Docker Compose
# ============================================================================
# Production-ready multi-service stack:
#   - Apache Spark (Master + Worker)
#   - JupyterLab with PySpark kernel
#   - Apache Airflow (Webserver + Scheduler + PostgreSQL metadata DB)
#
# Usage:
#   docker compose up -d             # Start all services
#   docker compose down               # Stop all services
#   docker compose logs -f <svc>      # Tail logs for a service
#   docker compose ps                 # List running services
# ============================================================================

# ---------------------------------------------------------------------------
# Named volumes
# ---------------------------------------------------------------------------
volumes:
  postgres-data:
    driver: local

# ---------------------------------------------------------------------------
# Shared network
# ---------------------------------------------------------------------------
networks:
  governance-net:
    driver: bridge

# ---------------------------------------------------------------------------
# Shared environment anchors (YAML anchors to keep DRY)
# ---------------------------------------------------------------------------
x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow
  image: governance-airflow:latest
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    AIRFLOW__WEBSERVER__RBAC: 'true'
    # -- Spark connection ---------------------------------------------------
    # Use local[*] so PySpark runs inside Airflow with pip-installed
    # Delta Lake JARs.  The standalone Spark cluster (spark-master /
    # spark-worker) is available for JupyterLab interactive work.
    SPARK_MASTER_URL: "local[*]"
    # -- Project paths inside the container ---------------------------------
    PYTHONPATH: /opt/framework
  volumes: &airflow-common-volumes
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./data:/opt/framework/data
    - ./src:/opt/framework/src
    - ./config:/opt/framework/config
    - ./models:/opt/framework/models
    - ./notebooks:/opt/framework/notebooks
    - ./scripts:/opt/framework/scripts
  networks:
    - governance-net

# ---------------------------------------------------------------------------
# Services
# ---------------------------------------------------------------------------
services:

  # =========================================================================
  # 1. Spark Master
  # =========================================================================
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    user: root
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master cluster port
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master --port 7077 --webui-port 8080
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/opt/framework
    volumes:
      - ./data:/opt/framework/data
      - ./src:/opt/framework/src
      - ./config:/opt/framework/config
      - ./models:/opt/framework/models
      - ./notebooks:/opt/framework/notebooks
    networks:
      - governance-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # =========================================================================
  # 2. Spark Worker
  # =========================================================================
  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    hostname: spark-worker
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8081 --memory 4G --cores 2
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_NO_DAEMONIZE=true
      - PYTHONPATH=/opt/framework
    volumes:
      - ./data:/opt/framework/data
      - ./src:/opt/framework/src
      - ./config:/opt/framework/config
      - ./models:/opt/framework/models
      - ./notebooks:/opt/framework/notebooks
    networks:
      - governance-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # =========================================================================
  # 3. JupyterLab (custom Dockerfile with PySpark kernel)
  # =========================================================================
  jupyterlab:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    container_name: jupyterlab
    hostname: jupyterlab
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8888:8888"   # JupyterLab Web UI
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=governance
      - SPARK_MASTER_URL=spark://spark-master:7077
      - PYSPARK_DRIVER_PYTHON=jupyter
      - PYSPARK_DRIVER_PYTHON_OPTS=lab
      - PYTHONPATH=/opt/framework
    volumes:
      - ./data:/opt/framework/data
      - ./src:/opt/framework/src
      - ./models:/opt/framework/models
      - ./notebooks:/opt/framework/notebooks
      - ./config:/opt/framework/config
      - ./scripts:/opt/framework/scripts
    networks:
      - governance-net
    restart: unless-stopped

  # =========================================================================
  # 4. PostgreSQL (Airflow metadata database)
  # =========================================================================
  postgres:
    image: postgres:14
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - governance-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # =========================================================================
  # 5. Airflow – one-shot init container (migrate DB + create admin user)
  # =========================================================================
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || true
        echo "✅  Airflow initialisation complete."
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"

  # =========================================================================
  # 6. Airflow Webserver
  # =========================================================================
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    hostname: airflow-webserver
    command: airflow webserver
    ports:
      - "8081:8080"   # Mapped to 8081 to avoid clash with Spark Master UI
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # =========================================================================
  # 7. Airflow Scheduler
  # =========================================================================
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
