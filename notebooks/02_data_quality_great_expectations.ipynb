{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517871c6",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 ¬∑ Setup & Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, warnings, json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='viridis', font_scale=1.1)\n",
    "plt.rcParams.update({'figure.figsize':(14,5), 'figure.dpi':110,\n",
    "                     'axes.titlesize':14, 'axes.titleweight':'bold'})\n",
    "\n",
    "from src.utils.spark_utils import get_spark_session\n",
    "spark = get_spark_session(app_name='NB02-DataQuality', master='local[*]', driver_memory='4g')\n",
    "print(f'‚úÖ SparkSession ready  ¬∑  v{spark.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52a2e5",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 ¬∑ Data Generation & Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_generator import generate_customers, generate_products, generate_orders\n",
    "\n",
    "print('‚è≥ Generating data ‚Ä¶')\n",
    "customers_pdf = generate_customers(n=10_000)\n",
    "products_pdf  = generate_products(n=2_000)\n",
    "orders_pdf    = generate_orders(\n",
    "    n=50_000,\n",
    "    customer_ids=customers_pdf['customer_id'].tolist(),\n",
    "    product_ids=products_pdf['product_id'].tolist(),\n",
    ")\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "orders_sdf    = spark.createDataFrame(orders_pdf)\n",
    "customers_sdf = spark.createDataFrame(customers_pdf)\n",
    "\n",
    "print(f'‚úÖ Orders: {orders_sdf.count():,} rows  ¬∑  Customers: {customers_sdf.count():,} rows')\n",
    "orders_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952622f5",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 ¬∑ Great Expectations ‚Äî ExpectationSuite\n",
    "\n",
    "The `DataQualityFramework` creates an 8-rule `ExpectationSuite` that encodes business rules:\n",
    "\n",
    "| # | Rule | GE Expectation |\n",
    "|---|------|----------------|\n",
    "| R1 | 9 required columns in order | `expect_table_columns_to_match_ordered_list` |\n",
    "| R2 | `order_id` not null | `expect_column_values_to_not_be_null` |\n",
    "| R3 | `customer_id` not null | `expect_column_values_to_not_be_null` |\n",
    "| R4 | `order_value` not null | `expect_column_values_to_not_be_null` |\n",
    "| R5 | `order_id` unique | `expect_column_values_to_be_unique` |\n",
    "| R6 | `order_value` ‚àà [0, 1M] (99 % mostly) | `expect_column_values_to_be_between` |\n",
    "| R7 | `payment_method` ‚àà valid set | `expect_column_values_to_be_in_set` |\n",
    "| R8 | `delivery_pincode` matches `^\\d{6}$` | `expect_column_values_to_match_regex` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quality.dq_framework import DataQualityFramework\n",
    "\n",
    "dqf = DataQualityFramework(spark)\n",
    "suite = dqf.create_ecommerce_expectations()\n",
    "\n",
    "print(f'‚úÖ ExpectationSuite created: \"{suite.expectation_suite_name}\"')\n",
    "print(f'   Number of expectations: {len(suite.expectations)}')\n",
    "print(f'\\n{\"‚îÄ\"*60}')\n",
    "for i, exp in enumerate(suite.expectations, 1):\n",
    "    exp_type = exp.expectation_type.replace('expect_', '').replace('_', ' ')\n",
    "    col = exp.kwargs.get('column', '‚Äî')\n",
    "    print(f'   R{i}: {exp_type:40s}  col={col}')\n",
    "print(f'{\"‚îÄ\"*60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20846a",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 ¬∑ GE Validation & Quarantine\n",
    "\n",
    "Records that fail validation are quarantined to a Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, shutil\n",
    "\n",
    "quarantine_path = os.path.join(tempfile.mkdtemp(), 'quarantine_orders')\n",
    "\n",
    "# The framework's validate_and_quarantine uses GE's SparkDFDataset\n",
    "# On GE >= 0.18 SparkDFDataset may be deprecated, so we handle gracefully\n",
    "try:\n",
    "    results = dqf.validate_and_quarantine(orders_sdf, suite, quarantine_path)\n",
    "    ge_ran = True\n",
    "    print('‚úÖ Great Expectations validation completed')\n",
    "    print(f'\\n   Total rows:     {results[\"metrics\"][\"total_rows\"]:>10,}')\n",
    "    print(f'   Valid rows:     {results[\"metrics\"][\"valid_rows\"]:>10,}')\n",
    "    print(f'   Failed rows:    {results[\"metrics\"][\"failed_rows\"]:>10,}')\n",
    "    print(f'   Success rate:   {results[\"metrics\"][\"success_rate\"]:>9.1f}%')\n",
    "\n",
    "    if results['metrics'].get('failed_rules'):\n",
    "        print(f'\\n   Failed Rules:')\n",
    "        for rule in results['metrics']['failed_rules']:\n",
    "            print(f'     ‚úó {rule}')\n",
    "\n",
    "except Exception as e:\n",
    "    ge_ran = False\n",
    "    print(f'‚ö†Ô∏è  GE SparkDFDataset not available in this environment: {e}')\n",
    "    print('   (This is expected with GE >= 0.18 ‚Äî SparkDFDataset was deprecated)')\n",
    "    print('   Proceeding with framework\\'s custom validation ‚Ä¶')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc555cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise GE results (or simulate if GE couldn't run)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if ge_ran:\n",
    "    valid_n  = results['metrics']['valid_rows']\n",
    "    failed_n = results['metrics']['failed_rows']\n",
    "    rate     = results['metrics']['success_rate']\n",
    "else:\n",
    "    # Manual rule checking as fallback\n",
    "    total = orders_sdf.count()\n",
    "    null_oid = orders_sdf.filter(F.col('order_id').isNull()).count()\n",
    "    null_cid = orders_sdf.filter(F.col('customer_id').isNull()).count()\n",
    "    null_val = orders_sdf.filter(F.col('order_value').isNull()).count()\n",
    "    neg_val  = orders_sdf.filter(F.col('order_value') < 0).count()\n",
    "    extreme  = orders_sdf.filter(F.col('order_value') > 1_000_000).count()\n",
    "    failed_n = null_oid + null_cid + null_val + neg_val + extreme\n",
    "    valid_n  = total - failed_n\n",
    "    rate     = valid_n / total * 100\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Donut chart\n",
    "ax1.pie([valid_n, failed_n], labels=['Valid', 'Failed'],\n",
    "        autopct='%1.1f%%', colors=['#27ae60', '#e74c3c'],\n",
    "        startangle=90, pctdistance=0.75,\n",
    "        wedgeprops=dict(width=0.4, edgecolor='white', linewidth=2))\n",
    "ax1.text(0, 0, f'{rate:.1f}%', ha='center', va='center',\n",
    "         fontsize=22, fontweight='bold', color='#2c3e50')\n",
    "ax1.set_title('GE Validation ‚Äî Pass Rate')\n",
    "\n",
    "# Rule-level breakdown\n",
    "rule_checks = {\n",
    "    'Null order_id':     int(orders_pdf['order_id'].isna().sum()),\n",
    "    'Null customer_id':  int(orders_pdf['customer_id'].isna().sum()),\n",
    "    'Null order_value':  int(orders_pdf['order_value'].isna().sum()),\n",
    "    'Negative value':    int((orders_pdf['order_value'] < 0).sum()),\n",
    "    'Extreme value':     int((orders_pdf['order_value'] > 1_000_000).sum()),\n",
    "    'Duplicate order_id':int(orders_pdf['order_id'].duplicated().sum()),\n",
    "}\n",
    "rule_df = pd.DataFrame(list(rule_checks.items()), columns=['Rule', 'Failures'])\n",
    "rule_df = rule_df.sort_values('Failures', ascending=True)\n",
    "rule_df.plot.barh(x='Rule', y='Failures', ax=ax2, color='#e74c3c', edgecolor='white', legend=False)\n",
    "ax2.set_xlabel('Failed Records')\n",
    "ax2.set_title('Rule-Level Failure Breakdown')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7300dc1",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 ¬∑ QualityMetrics ‚Äî 5-Dimension Scoring\n",
    "\n",
    "The framework measures data quality across **5 ISO 25012 dimensions**:\n",
    "\n",
    "| Dimension | Description | Method |\n",
    "|-----------|-------------|--------|\n",
    "| **Completeness** | % non-null values | `measure_completeness` |\n",
    "| **Uniqueness** | % distinct values | `measure_uniqueness` |\n",
    "| **Validity** | % values passing rules | `measure_validity` |\n",
    "| **Timeliness** | Freshness of data | `measure_timeliness` |\n",
    "| **Consistency** | Cross-field rule adherence | `measure_consistency` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quality.quality_metrics import QualityMetrics\n",
    "\n",
    "qm = QualityMetrics(spark)\n",
    "\n",
    "# Define validity rules for orders\n",
    "validity_rules = {\n",
    "    'order_value >= 0':               'order_value >= 0',\n",
    "    'order_value <= 1000000':          'order_value <= 1000000',\n",
    "    'order_id IS NOT NULL':            'order_id IS NOT NULL',\n",
    "    'customer_id IS NOT NULL':         'customer_id IS NOT NULL',\n",
    "}\n",
    "\n",
    "# Consistency rules\n",
    "consistency_rules = {\n",
    "    'delivery_date >= order_timestamp': 'delivery_date >= order_timestamp',\n",
    "}\n",
    "\n",
    "# Compute overall quality score\n",
    "quality_report = qm.compute_overall_score(\n",
    "    df=orders_sdf,\n",
    "    table_name='orders',\n",
    "    validity_rules=validity_rules,\n",
    "    consistency_rules=consistency_rules,\n",
    "    timestamp_col='order_timestamp',\n",
    ")\n",
    "\n",
    "overall = quality_report['overall_score']\n",
    "dims    = quality_report['dimensions']\n",
    "\n",
    "print(f'\\n{\"‚ïê\"*60}')\n",
    "print(f'  QUALITY SCORE: {overall:.1f} / 100')\n",
    "print(f'{\"‚ïê\"*60}')\n",
    "for dim, score in dims.items():\n",
    "    bar = '‚ñà' * int(score / 5) + '‚ñë' * (20 - int(score / 5))\n",
    "    print(f'  {dim:15s}  {bar}  {score:.1f}%')\n",
    "print(f'{\"‚ïê\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart of 5 dimensions\n",
    "dim_names  = list(dims.keys())\n",
    "dim_values = list(dims.values())\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(dim_names), endpoint=False).tolist()\n",
    "dim_values_plot = dim_values + [dim_values[0]]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))\n",
    "ax.fill(angles, dim_values_plot, alpha=0.25, color='#3498db')\n",
    "ax.plot(angles, dim_values_plot, 'o-', linewidth=2, color='#2980b9', markersize=8)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(dim_names, fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_title(f'Data Quality Radar ‚Äî Overall {overall:.1f}%', fontsize=14,\n",
    "             fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47951d",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6 ¬∑ Anomaly Detection ‚Äî Z-Score\n",
    "\n",
    "Statistical Z-score method: flags values > 3 standard deviations from mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quality.anomaly_detector import AnomalyDetector\n",
    "\n",
    "ad = AnomalyDetector(spark, z_threshold=3.0, iqr_factor=1.5, contamination=0.05)\n",
    "\n",
    "# Z-Score detection\n",
    "zscore_df = ad.z_score_detection(orders_sdf, column='order_value')\n",
    "z_anomalies = zscore_df.filter(F.col('_is_anomaly_zscore') == True)\n",
    "z_count = z_anomalies.count()\n",
    "z_total = zscore_df.count()\n",
    "\n",
    "print(f'üî¨ Z-Score Anomaly Detection (threshold = 3.0œÉ)')\n",
    "print(f'   Total rows:    {z_total:>10,}')\n",
    "print(f'   Anomalies:     {z_count:>10,}  ({z_count/z_total*100:.2f}%)')\n",
    "\n",
    "# Plot\n",
    "z_pdf = zscore_df.select('order_value', '_is_anomaly_zscore').toPandas()\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "normal = z_pdf[~z_pdf['_is_anomaly_zscore']]\n",
    "anomal = z_pdf[z_pdf['_is_anomaly_zscore']]\n",
    "ax.scatter(range(len(normal)), normal['order_value'], s=1, alpha=0.3, c='#3498db', label='Normal')\n",
    "ax.scatter(range(len(normal), len(normal)+len(anomal)), anomal['order_value'],\n",
    "           s=8, alpha=0.6, c='#e74c3c', label='Anomaly')\n",
    "ax.set_ylabel('Order Value (‚Çπ)')\n",
    "ax.set_title(f'Z-Score Anomaly Detection ‚Äî {z_count:,} anomalies flagged')\n",
    "ax.legend()\n",
    "ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('‚Çπ{x:,.0f}'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc0e22",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7 ¬∑ Anomaly Detection ‚Äî IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR detection\n",
    "iqr_df = ad.iqr_detection(orders_sdf, column='order_value')\n",
    "iqr_anomalies = iqr_df.filter(F.col('_is_anomaly_iqr') == True)\n",
    "iqr_count = iqr_anomalies.count()\n",
    "\n",
    "print(f'üî¨ IQR Anomaly Detection (factor = 1.5)')\n",
    "print(f'   Total rows:    {z_total:>10,}')\n",
    "print(f'   Anomalies:     {iqr_count:>10,}  ({iqr_count/z_total*100:.2f}%)')\n",
    "\n",
    "iqr_pdf = iqr_df.select('order_value', '_is_anomaly_iqr').toPandas()\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "norm = iqr_pdf[~iqr_pdf['_is_anomaly_iqr']]\n",
    "anom = iqr_pdf[iqr_pdf['_is_anomaly_iqr']]\n",
    "ax.scatter(range(len(norm)), norm['order_value'], s=1, alpha=0.3, c='#3498db', label='Normal')\n",
    "ax.scatter(range(len(norm), len(norm)+len(anom)), anom['order_value'],\n",
    "           s=8, alpha=0.6, c='#e74c3c', label='Anomaly')\n",
    "ax.set_ylabel('Order Value (‚Çπ)')\n",
    "ax.set_title(f'IQR Anomaly Detection ‚Äî {iqr_count:,} anomalies flagged')\n",
    "ax.legend()\n",
    "ax.yaxis.set_major_formatter(mtick.StrMethodFormatter('‚Çπ{x:,.0f}'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28160b6",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8 ¬∑ Anomaly Detection ‚Äî Isolation Forest\n",
    "\n",
    "ML-based unsupervised anomaly detection using scikit-learn's `IsolationForest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest (operates on numeric columns)\n",
    "if_df = ad.isolation_forest_detection(\n",
    "    orders_sdf,\n",
    "    columns=['order_value'],\n",
    "    contamination=0.05,\n",
    ")\n",
    "if_anomalies = if_df.filter(F.col('_is_anomaly_iforest') == True)\n",
    "if_count = if_anomalies.count()\n",
    "\n",
    "print(f'üî¨ Isolation Forest Detection (contamination = 0.05)')\n",
    "print(f'   Total rows:    {z_total:>10,}')\n",
    "print(f'   Anomalies:     {if_count:>10,}  ({if_count/z_total*100:.2f}%)')\n",
    "\n",
    "if_pdf = if_df.select('order_value', '_is_anomaly_iforest', '_anomaly_score').toPandas()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "norm_if = if_pdf[~if_pdf['_is_anomaly_iforest']]\n",
    "anom_if = if_pdf[if_pdf['_is_anomaly_iforest']]\n",
    "ax1.scatter(range(len(norm_if)), norm_if['order_value'], s=1, alpha=0.3, c='#3498db', label='Normal')\n",
    "ax1.scatter(range(len(norm_if), len(norm_if)+len(anom_if)), anom_if['order_value'],\n",
    "            s=8, alpha=0.6, c='#e74c3c', label='Anomaly')\n",
    "ax1.set_ylabel('Order Value (‚Çπ)')\n",
    "ax1.set_title(f'Isolation Forest ‚Äî {if_count:,} anomalies')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(if_pdf['_anomaly_score'], bins=60, color='#9b59b6', edgecolor='white', alpha=0.8)\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Decision boundary')\n",
    "ax2.set_xlabel('Anomaly Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Anomaly Score Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639350e0",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9 ¬∑ Anomaly Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280071",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Method':    ['Z-Score', 'IQR', 'Isolation Forest'],\n",
    "    'Anomalies': [z_count, iqr_count, if_count],\n",
    "    'Rate (%)':  [z_count/z_total*100, iqr_count/z_total*100, if_count/z_total*100],\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors_method = ['#3498db', '#f39c12', '#9b59b6']\n",
    "ax1.bar(comparison['Method'], comparison['Anomalies'], color=colors_method, edgecolor='white')\n",
    "ax1.set_ylabel('Anomalies Detected')\n",
    "ax1.set_title('Anomaly Count by Method')\n",
    "for i, v in enumerate(comparison['Anomalies']):\n",
    "    ax1.text(i, v + 20, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "ax2.bar(comparison['Method'], comparison['Rate (%)'], color=colors_method, edgecolor='white')\n",
    "ax2.set_ylabel('Anomaly Rate (%)')\n",
    "ax2.set_title('Anomaly Rate by Method')\n",
    "for i, v in enumerate(comparison['Rate (%)']):\n",
    "    ax2.text(i, v + 0.05, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(comparison.style.set_caption('üî¨ Anomaly Detection ‚Äî Method Comparison')\n",
    "        .format({'Anomalies':'{:,}', 'Rate (%)':'{:.2f}%'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6e026",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10 ¬∑ AdaptiveDQScorer ‚Äî Weight Learning\n",
    "\n",
    "The `AdaptiveDQScorer` uses historical quality scores to **learn optimal dimension weights** via least-squares regression, and adaptively sets pass/fail thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.quality.adaptive_scorer import AdaptiveDQScorer\n",
    "\n",
    "scorer = AdaptiveDQScorer(\n",
    "    history_dir='data/metrics/adaptive',\n",
    "    baseline_window=20,\n",
    "    sensitivity=1.5,\n",
    ")\n",
    "\n",
    "# Feed the quality report into the scorer\n",
    "adaptive_result = scorer.score(quality_report)\n",
    "\n",
    "print(f'\\n{\"‚ïê\"*60}')\n",
    "print(f'  ADAPTIVE DQ SCORE: {adaptive_result[\"overall_score\"]:.1f}')\n",
    "print(f'  Status:            {adaptive_result[\"status\"]}')\n",
    "print(f'  Threshold:         {adaptive_result[\"threshold\"]:.1f}')\n",
    "print(f'  History count:     {adaptive_result[\"history_count\"]}')\n",
    "print(f'{\"‚ïê\"*60}')\n",
    "print(f'\\n  Adaptive Weights (learned from history):')\n",
    "for dim, weight in adaptive_result['weights'].items():\n",
    "    pct = weight * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f'    {dim:15s}  {bar:20s}  {pct:.1f}%')\n",
    "\n",
    "# Detect trend\n",
    "trend = scorer.detect_trend(adaptive_result)\n",
    "print(f'\\n  Trend:  {trend[\"trend\"]}  (severity: {trend[\"severity\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise adaptive weights\n",
    "weights = adaptive_result['weights']\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Weight pie chart\n",
    "ax1.pie(weights.values(), labels=weights.keys(), autopct='%1.1f%%',\n",
    "        colors=sns.color_palette('Set2', len(weights)),\n",
    "        startangle=140, wedgeprops=dict(edgecolor='white', linewidth=1.5))\n",
    "ax1.set_title('Adaptive Dimension Weights')\n",
    "\n",
    "# Score vs threshold gauge\n",
    "score = adaptive_result['overall_score']\n",
    "thresh = adaptive_result['threshold']\n",
    "ax2.barh(['Score'], [score], color='#27ae60' if score >= thresh else '#e74c3c',\n",
    "         edgecolor='white', height=0.4)\n",
    "ax2.axvline(x=thresh, color='orange', linewidth=3, linestyle='--', label=f'Threshold ({thresh:.0f})')\n",
    "ax2.set_xlim(0, 100)\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_title(f'Adaptive Score: {score:.1f} vs Threshold: {thresh:.0f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61efc10",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11 ¬∑ Data Contract Enforcement\n",
    "\n",
    "YAML-based data contracts define schema and business rules. The `ContractEnforcer` splits data into valid and quarantined DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.governance.data_contracts import ContractRegistry, ContractEnforcer\n",
    "\n",
    "registry = ContractRegistry(contracts_dir='config/data_contracts')\n",
    "\n",
    "# List registered contracts\n",
    "contracts = registry.list_all()\n",
    "print(f'üìú Registered Data Contracts: {len(contracts)}')\n",
    "for c in contracts:\n",
    "    print(f'   ‚Ä¢ {c[\"name\"]} v{c[\"version\"]}  ({c.get(\"description\", \"\")})')\n",
    "\n",
    "# Enforce on orders data\n",
    "enforcer = ContractEnforcer(spark, registry=registry, quarantine_path='data/quarantine')\n",
    "\n",
    "try:\n",
    "    valid_df, quarantined_df, report = enforcer.enforce(orders_sdf, 'ecommerce_orders')\n",
    "    v_count = valid_df.count()\n",
    "    q_count = quarantined_df.count()\n",
    "\n",
    "    print(f'\\n‚úÖ Contract Enforcement Complete')\n",
    "    print(f'   Valid records:       {v_count:>10,}')\n",
    "    print(f'   Quarantined records: {q_count:>10,}')\n",
    "    print(f'   Pass rate:           {v_count / (v_count + q_count) * 100:>9.1f}%')\n",
    "\n",
    "    if report.get('violations'):\n",
    "        print(f'\\n   Violations:')\n",
    "        for v in report['violations'][:10]:\n",
    "            print(f'     ‚úó {v}')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Contract enforcement note: {e}')\n",
    "    print('   (Contract may expect different column names than raw data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc61967",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## 12 ¬∑ Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd895211",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'''\n",
    "<div style=\"background:linear-gradient(135deg,#0f3460,#16213e,#1a1a2e);color:white;\n",
    "            padding:30px;border-radius:12px;font-family:sans-serif;\">\n",
    "  <h2 style=\"text-align:center;margin:0 0 20px;\">üîç Data Quality ‚Äî Executive Summary</h2>\n",
    "  <div style=\"display:grid;grid-template-columns:repeat(3,1fr);gap:15px;\">\n",
    "    <div style=\"background:rgba(39,174,96,0.2);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">OVERALL DQ SCORE</div>\n",
    "      <div style=\"font-size:32px;font-weight:bold;color:#27ae60;\">{overall:.1f}%</div></div>\n",
    "    <div style=\"background:rgba(255,255,255,0.08);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">GE EXPECTATIONS</div>\n",
    "      <div style=\"font-size:32px;font-weight:bold;\">8 rules</div></div>\n",
    "    <div style=\"background:rgba(255,255,255,0.08);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">ADAPTIVE STATUS</div>\n",
    "      <div style=\"font-size:32px;font-weight:bold;color:{'#27ae60' if adaptive_result['status']=='PASS' else '#e74c3c'};\">\n",
    "        {adaptive_result['status']}</div></div>\n",
    "    <div style=\"background:rgba(52,152,219,0.2);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">Z-SCORE ANOMALIES</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">{z_count:,}</div></div>\n",
    "    <div style=\"background:rgba(243,156,18,0.2);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">IQR ANOMALIES</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">{iqr_count:,}</div></div>\n",
    "    <div style=\"background:rgba(155,89,182,0.2);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">ISOLATION FOREST</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">{if_count:,}</div></div>\n",
    "    <div style=\"background:rgba(255,255,255,0.08);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">QUALITY DIMENSIONS</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">5</div></div>\n",
    "    <div style=\"background:rgba(255,255,255,0.08);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">ANOMALY METHODS</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">3</div></div>\n",
    "    <div style=\"background:rgba(255,255,255,0.08);padding:18px;border-radius:10px;text-align:center;\">\n",
    "      <div style=\"font-size:11px;opacity:0.7;\">DATA CONTRACTS</div>\n",
    "      <div style=\"font-size:28px;font-weight:bold;\">{len(contracts)}</div></div>\n",
    "  </div>\n",
    "  <p style=\"text-align:center;margin:20px 0 0;opacity:0.6;font-size:12px;\">\n",
    "    Proceed to <b>Notebook 03</b> for PII Detection & Privacy analysis</p>\n",
    "</div>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a6cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "try:\n",
    "    shutil.rmtree(os.path.dirname(quarantine_path), ignore_errors=True)\n",
    "except:\n",
    "    pass\n",
    "spark.stop()\n",
    "print('‚úÖ SparkSession stopped ‚Äî Notebook 02 complete')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
