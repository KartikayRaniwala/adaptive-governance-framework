#!/bin/bash
# ============================================================================
# Adaptive Governance Framework - Deployment Script
# ============================================================================
# Single-shot deployment: creates directories, copies env, builds images,
# starts all services, waits for health checks, unpauses DAG, and triggers
# the full pipeline automatically.
#
# Usage:  bash scripts/deploy.sh          (from project root)
#         bash scripts/deploy.sh --build   (force rebuild images)
#         bash scripts/deploy.sh --down    (tear down everything)
# ============================================================================

set -euo pipefail

# ---------- Colour helpers ------------------------------------------------
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m'  # No colour

info()  { echo -e "${CYAN}[INFO]${NC}  $*"; }
ok()    { echo -e "${GREEN}[  OK]${NC}  $*"; }
warn()  { echo -e "${YELLOW}[WARN]${NC}  $*"; }
fail()  { echo -e "${RED}[FAIL]${NC}  $*"; exit 1; }

# ---------- Resolve project root ------------------------------------------
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
cd "$PROJECT_ROOT"
info "Project root: $PROJECT_ROOT"

# ---------- Parse arguments ------------------------------------------------
if [[ "${1:-}" == "--down" ]]; then
    info "Tearing down all services …"
    docker compose down -v --remove-orphans 2>/dev/null || true
    ok "All services stopped."
    exit 0
fi

# ---------- Pre-flight checks ---------------------------------------------
echo ""
echo "========================================="
echo " Adaptive Governance Framework"
echo " Deployment"
echo "========================================="
echo ""

# Docker daemon running?
if ! docker info >/dev/null 2>&1; then
    fail "Docker is not running. Please start Docker Desktop and retry."
fi
ok "Docker daemon is running."

# docker compose available?
if ! docker compose version >/dev/null 2>&1; then
    fail "'docker compose' plugin not found. Install Docker Compose V2."
fi
ok "Docker Compose V2 detected: $(docker compose version --short 2>/dev/null || echo 'unknown')"

# ---------- Create required directories -----------------------------------
info "Ensuring directory structure …"
dirs=(
    data/raw
    data/bronze
    data/silver
    data/gold
    data/quarantine
    data/metrics
    data/metrics/adaptive
    data/metrics/anomaly_history
    data/metrics/pii_feedback
    data/metrics/governance_reports
    data/reports/pii_audit
    data/streaming/landing
    data/streaming/_checkpoints
    data/stress_test
    config/data_mesh
    config/data_contracts
    config/stress_test
    airflow/dags
    airflow/logs
    airflow/plugins
    models
    notebooks
    logs
)
for d in "${dirs[@]}"; do
    mkdir -p "$d"
done
ok "All directories created."

# ---------- Environment file ----------------------------------------------
if [ ! -f .env ]; then
    if [ -f .env.example ]; then
        cp .env.example .env
        ok "Created .env from .env.example"
    else
        # Create minimal .env
        cat > .env << 'ENVEOF'
# Auto-generated by deploy.sh
AIRFLOW_UID=50000
AIRFLOW_GID=0
ENVEOF
        ok "Created minimal .env"
    fi
else
    ok ".env already exists."
fi

# ---------- Fix permissions (Airflow needs writable logs) -----------------
chmod -R 777 airflow/logs 2>/dev/null || true

# ---------- Clean previous data (fresh run) -------------------------------
info "Cleaning previous data layers for fresh run …"
rm -rf data/raw/* data/bronze/* data/silver/* data/gold/* data/quarantine/*
rm -rf data/streaming/landing/* data/streaming/_checkpoints/*
rm -rf data/metrics/adaptive/* data/metrics/anomaly_history/*
rm -rf data/metrics/governance_reports/*
ok "Data layers cleaned."

# ---------- Stop existing containers (clean slate) ------------------------
info "Stopping any existing containers …"
docker compose down --remove-orphans 2>/dev/null || true

# ---------- Build images --------------------------------------------------
info "Building Docker images (this may take a few minutes on first run) …"
docker compose build 2>&1 | tail -5
ok "Images built."

# ---------- Start PostgreSQL first ----------------------------------------
info "Starting PostgreSQL …"
docker compose up -d postgres
ok "PostgreSQL container started."

# Wait for PostgreSQL health
info "Waiting for PostgreSQL to be healthy …"
RETRIES=30
until docker compose exec -T postgres pg_isready -U airflow >/dev/null 2>&1; do
    RETRIES=$((RETRIES - 1))
    if [ $RETRIES -le 0 ]; then fail "PostgreSQL did not become healthy in time."; fi
    sleep 2
done
ok "PostgreSQL is healthy."

# ---------- Start Airflow init (DB migration + admin user) ----------------
info "Running Airflow database migration …"
docker compose up airflow-init 2>&1 | tail -3
ok "Airflow initialisation complete."

# ---------- Start remaining services --------------------------------------
info "Starting all services …"
docker compose up -d
ok "All containers started."

# ---------- Health-check loop ---------------------------------------------
info "Waiting for services to become healthy (up to 120s) …"

wait_for_url() {
    local name="$1" url="$2" max_wait="${3:-90}"
    local elapsed=0
    while ! curl -sf "$url" >/dev/null 2>&1; do
        elapsed=$((elapsed + 3))
        if [ $elapsed -ge $max_wait ]; then
            warn "$name did not respond at $url within ${max_wait}s"
            return 1
        fi
        sleep 3
    done
    ok "$name is ready ($url)"
    return 0
}

wait_for_url "Spark Master UI"   "http://localhost:8080"  90
wait_for_url "Airflow Webserver"  "http://localhost:8081/health" 120
wait_for_url "JupyterLab"        "http://localhost:8888"  90

# ---------- Clear stale pyc cache -----------------------------------------
info "Clearing Python cache in containers …"
docker compose exec -T airflow-scheduler bash -c "find /opt -name '__pycache__' -exec rm -rf {} + 2>/dev/null; find /opt -name '*.pyc' -delete 2>/dev/null" || true
ok "Cache cleared."

# ---------- Unpause and Trigger DAG ----------------------------------------
info "Unpausing and triggering medallion_pipeline_dag …"

# Wait a bit for scheduler to detect DAGs
sleep 10

docker compose exec -T airflow-scheduler airflow dags unpause medallion_pipeline_dag 2>/dev/null || true
sleep 2
docker compose exec -T airflow-scheduler airflow dags unpause pii_audit_dag 2>/dev/null || true

# Trigger the main pipeline
docker compose exec -T airflow-scheduler airflow dags trigger medallion_pipeline_dag 2>/dev/null || true
ok "Pipeline triggered!"

# ---------- Show running containers ----------------------------------------
echo ""
info "Container status:"
docker compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}" 2>/dev/null || docker compose ps

# ---------- Summary -------------------------------------------------------
echo ""
echo "========================================="
echo -e "${GREEN} Deployment Complete!${NC}"
echo "========================================="
echo ""
echo "Access Points:"
echo "  - Spark Master UI:    http://localhost:8080"
echo "  - JupyterLab:         http://localhost:8888  (token: governance)"
echo "  - Airflow UI:         http://localhost:8081  (admin / admin)"
echo "  - PostgreSQL:         localhost:5432         (airflow / airflow)"
echo ""
echo "Pipeline Status:"
echo "  The medallion_pipeline_dag has been triggered automatically."
echo "  Monitor progress at: http://localhost:8081"
echo ""
echo "Expected Pipeline Flow:"
echo "  1. generate_synthetic_data  — 100K customers, 500K orders, etc."
echo "  2. ingest_to_bronze         — Raw → Bronze Delta Lake"
echo "  3. streaming_ingestion      — 20K clickstream events (parallel)"
echo "  3. bronze_to_silver         — PII masking + quarantine (parallel)"
echo "  4. data_quality_check       — Adaptive AI-driven DQ gate"
echo "  5. silver_to_gold           — Aggregations + identity resolution"
echo "  6. pii_scan_summary         — Post-masking PII verification"
echo "  7. log_completion           — Full summary with all metrics"
echo ""
echo "Useful commands:"
echo "  docker compose logs -f airflow-scheduler    # Tail pipeline logs"
echo "  docker compose logs -f spark-master         # Tail Spark logs"
echo "  docker compose down                         # Stop everything"
echo "  bash scripts/deploy.sh --down               # Stop + remove volumes"
echo ""
echo "========================================="
